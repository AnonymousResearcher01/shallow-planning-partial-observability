{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook for the paper titled \"Shallow Planning with Partial Observability\"\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mdptoolbox\n",
    "import time\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedMdp:\n",
    "    \"\"\"\n",
    "    A class to represent a fixed Markov Decision Process (MDP).\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    num_states : int\n",
    "        Number of states in the MDP.\n",
    "    num_actions : int\n",
    "        Number of actions in the MDP.\n",
    "    d : int\n",
    "        Number of possible next states for each state-action pair.\n",
    "    transitions : np.ndarray\n",
    "        Transition probabilities for the MDP.\n",
    "    rewards : np.ndarray\n",
    "        Rewards for the MDP.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    generate_mdp():\n",
    "        Generates the transition probabilities and rewards for an mdp of the type fixed. See the paper for details.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states=10, d=3):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = 2\n",
    "        self.d = d\n",
    "        self.transitions = np.zeros((self.num_actions, num_states, num_states))\n",
    "        self.rewards = np.zeros((num_states, self.num_actions))\n",
    "        self.generate_mdp()\n",
    "\n",
    "    def generate_mdp(self):\n",
    "        for state in range(self.num_states):\n",
    "            for action in range(self.num_actions):\n",
    "                #Each state can be connected to d other states. We just fill non empty entries with transitions and rewards.\n",
    "                probabilities = np.random.uniform(0, 1, self.d)\n",
    "                probabilities /= probabilities.sum()\n",
    "                next_states = np.random.choice(self.num_states, self.d, replace=False)\n",
    "                self.rewards[state, action] = np.random.uniform(0, 1)\n",
    "                self.transitions[action, state, next_states] = probabilities\n",
    "\n",
    "def generate_mdps(num_mdps=1000, num_states=10, d=3):\n",
    "    return [FixedMdp(num_states=num_states, d=d) for _ in range(num_mdps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingMdp:\n",
    "    \"\"\"\n",
    "    A class to represent a ring-shaped Markov Decision Process (MDP).\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    num_states : int\n",
    "        Number of states in the MDP.\n",
    "    p : float\n",
    "        Probability of additional random connections.\n",
    "    num_actions : int\n",
    "        Number of actions in the MDP.\n",
    "    transitions : np.ndarray\n",
    "        Transition probabilities for the MDP.\n",
    "    rewards : np.ndarray\n",
    "        Rewards for the MDP.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    generate_mdp():\n",
    "        Generates the transition probabilities and rewards for an mdp of type ring. See the paper for details.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states=10, p=0):\n",
    "        self.num_states = num_states\n",
    "        self.p = p\n",
    "        self.num_actions = 2\n",
    "        self.transitions = np.zeros((2, num_states, num_states))\n",
    "        self.rewards = np.zeros((num_states, 2))\n",
    "        self.generate_mdp()\n",
    "\n",
    "    def generate_mdp(self):\n",
    "        for state in range(self.num_states):\n",
    "            next_state_clockwise = (state + 1) % self.num_states\n",
    "            prev_state_counterclockwise = (state - 1) % self.num_states\n",
    "            \n",
    "            # Action 1: Move clockwise or stay\n",
    "            self.transitions[0, state, [next_state_clockwise, state]] = np.random.uniform(0, 1, 2)\n",
    "\n",
    "            # Action 2: Move counterclockwise or stay\n",
    "            self.transitions[1, state, [prev_state_counterclockwise, state]] = np.random.uniform(0, 1, 2)\n",
    "            \n",
    "            # Additional random connections based on probability p\n",
    "            for other_state in range(self.num_states):\n",
    "                if other_state != state and other_state != next_state_clockwise and np.random.rand() < self.p:\n",
    "                    self.transitions[0, state, other_state] = np.random.uniform(0, 1)\n",
    "                if other_state != state and other_state != prev_state_counterclockwise and np.random.rand() < self.p:\n",
    "                    self.transitions[1, state, other_state] = np.random.uniform(0, 1)\n",
    "            \n",
    "            # Normalize again after adding random connections\n",
    "            self.transitions[0, state, :] /= self.transitions[0, state, :].sum()\n",
    "            self.transitions[1, state, :] /= self.transitions[1, state, :].sum()\n",
    "            \n",
    "            # Generate rewards\n",
    "            self.rewards[state, 0] = np.random.uniform(0, 1)\n",
    "            self.rewards[state, 1] = np.random.uniform(0, 1)\n",
    "\n",
    "def generate_ring_mdps(num_mdps=1000, num_states=10, p=0):\n",
    "    return [RingMdp(num_states, p) for _ in range(num_mdps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDP:\n",
    "    \"\"\"\n",
    "    Partially Observable Markov Decision Process (POMDP) class.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mdp : RingMdp\n",
    "        The underlying Markov Decision Process (MDP).\n",
    "    num_observations : int\n",
    "        Number of possible observations.\n",
    "    num_actions : int\n",
    "        Number of possible actions.\n",
    "    p_o_s : np.ndarray\n",
    "        Observation probabilities given states.\n",
    "    p_s_o : np.ndarray\n",
    "        State probabilities given observations.\n",
    "    transitions : np.ndarray\n",
    "        Transition probabilities between observations.\n",
    "    rewards : np.ndarray\n",
    "        Rewards for each observation-action pair.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    define_observation_prob():\n",
    "        Defines the observation probabilities given states.\n",
    "    define_p_s_o():\n",
    "        Defines the state probabilities given observations.\n",
    "    define_transitions():\n",
    "        Defines the transition probabilities between observations.\n",
    "    define_rewards():\n",
    "        Defines the rewards for each observation-action pair.\n",
    "    transition_function(current_state, action):\n",
    "        Simulates a transition based on the MDP's transition probabilities.\n",
    "    observation_function(next_state, action):\n",
    "        Generates an observation based on the state's observation probability distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, mdp, num_observations):\n",
    "        \"\"\"\n",
    "        Initializes the POMDP with the given MDP and number of observations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mdp : RingMdp\n",
    "            The underlying Markov Decision Process (MDP).\n",
    "        num_observations : int\n",
    "            Number of possible observations.\n",
    "        \"\"\"\n",
    "        self.mdp = mdp\n",
    "        self.num_observations = num_observations\n",
    "        self.num_actions = mdp.num_actions\n",
    "        self.define_p_o_s()\n",
    "        self.define_p_s_o()\n",
    "        self.define_transitions()\n",
    "        self.define_rewards()\n",
    "\n",
    "    def define_p_o_s(self):\n",
    "        \"\"\"\n",
    "        Defines the observation probabilities given states.\n",
    "        \"\"\"\n",
    "        # Initialize P(O|S) for each state as a matrix of zeros\n",
    "        self.p_o_s = np.zeros((self.mdp.num_states, self.num_observations))\n",
    "        \n",
    "        # Assign each state to an observation ensuring every observation has at least one state\n",
    "        for state in range(self.mdp.num_states):\n",
    "            self.p_o_s[state, state % self.num_observations] = 1\n",
    "\n",
    "    def define_p_s_o(self):\n",
    "        \"\"\"\n",
    "        Defines the state probabilities given observations.\n",
    "        \"\"\"\n",
    "        # Initialize P(S|O) for each observation as a matrix of zeros\n",
    "        self.p_s_o = np.zeros((self.num_observations, self.mdp.num_states))\n",
    "        # Here for each observation we just define a uniform distribution on the corresponding ground states\n",
    "        for obs in range(self.num_observations):\n",
    "            ground_states = np.where(self.p_o_s[:, obs] == 1)[0]\n",
    "            num_ground_states = len(ground_states)\n",
    "            self.p_s_o[obs, :] = 0  # Set all probabilities to zero\n",
    "            self.p_s_o[obs, ground_states] = 1 / num_ground_states  # Set uniform probability for relevant ground states\n",
    "        \n",
    "    def define_transitions(self):\n",
    "        \"\"\"\n",
    "        Defines the transition probabilities between observations.\n",
    "        \"\"\"\n",
    "        self.transitions = np.zeros((self.num_actions, self.num_observations, self.num_observations))\n",
    "\n",
    "        for action in range(self.num_actions):\n",
    "            for observation in range(self.num_observations):\n",
    "                for next_observation in range(self.num_observations):\n",
    "                    self.transitions[action, observation, next_observation] = np.sum([np.dot(self.p_s_o[observation, :], self.mdp.transitions[action, :, next_state]) for next_state in range(self.mdp.num_states) if self.p_s_o[next_observation, next_state] > 0])\n",
    "\n",
    "    def define_rewards(self):\n",
    "        \"\"\"\n",
    "        Defines the rewards for each observation-action pair.\n",
    "        \"\"\"\n",
    "        self.rewards = np.einsum('os,sa->oa', self.p_s_o, self.mdp.rewards)\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Simulates a transition based on the MDP's transition probabilities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state : int\n",
    "            The current state in the MDP.\n",
    "        action : int\n",
    "            The action taken.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The next state in the MDP.\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.mdp.num_states, p=self.mdp.transitions[action, current_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tightness of theorem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this section, we will use the fixed MDP but this can be easily adapted to ring MDP or any other MDP.\n",
    "\n",
    "#We assume the rewards are known for our certainty equivalence setting similar to (The Dependence of Effective Planning Horizon on Model Accuracy Jiang et al 2015)\n",
    "#We compute approximate transitions from n_sample and deduce the optimal policy to compare the bound.\n",
    "\n",
    "class EstimatedMDP:\n",
    "    def __init__(self, transitions, rewards, num_states, d):\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.num_states = num_states\n",
    "        self.d = d\n",
    "        self.num_actions = 2  # Assuming a fixed number of actions as in the FixedMdp class\n",
    "\n",
    "\n",
    "def estimate_single_mdp(mdp, n_samples):\n",
    "    \"\"\"\n",
    "    Estimate the transition probabilities of a single MDP using sample transitions.\n",
    "\n",
    "    Parameters:\n",
    "    mdp (MDP): The Markov Decision Process to estimate.\n",
    "    n_samples (int): The number of samples to use for estimating the transitions.\n",
    "\n",
    "    Returns:\n",
    "    EstimatedMDP: A new MDP with estimated transition probabilities.\n",
    "    \"\"\"\n",
    "    num_states = mdp.num_states\n",
    "    num_actions = mdp.num_actions\n",
    "    estimated_transitions = np.zeros((num_actions, num_states, num_states))\n",
    "    \n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            next_states = np.random.choice(num_states, size=n_samples, p=mdp.transitions[action, state, :])\n",
    "            \n",
    "            for next_state in range(num_states):\n",
    "                estimated_transitions[action, state, next_state] = np.sum(next_states == next_state) / n_samples\n",
    "\n",
    "    return EstimatedMDP(estimated_transitions, mdp.rewards, num_states, num_actions)\n",
    "\n",
    "#This function exists for paralellization purposes\n",
    "def estimate_mdps(mdps, n_samples):\n",
    "    estimated_mdps = Parallel(n_jobs=-1)(delayed(estimate_single_mdp)(mdp, n_samples) for mdp in mdps)\n",
    "    return estimated_mdps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta_M(mdp, discordant_states):\n",
    "    \"\"\"\n",
    "    Calculate delta_M over discordant states. \n",
    "\n",
    "    Parameters:\n",
    "    mdp (MDP): The Markov Decision Process.\n",
    "    state_vector (np.array): A vector of 1s and 0s indicating the states to consider.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An array of delta_M values for the states indicated by the state_vector.\n",
    "    \"\"\"\n",
    "    num_states = mdp.num_states\n",
    "    num_actions = mdp.num_actions\n",
    "    delta_M = 0\n",
    "\n",
    "    for state in range(num_states):\n",
    "        if discordant_states[state] == 1:\n",
    "            if np.sum(np.abs(mdp.transitions[0, state, :] - mdp.transitions[1, state, :])) > delta_M:\n",
    "                delta_M = np.sum(np.abs(mdp.transitions[0, state, :] - mdp.transitions[1, state, :]))\n",
    "\n",
    "    return delta_M\n",
    "\n",
    "def bias_ad_hoc_function(delta_M, blackwell_gamma):\n",
    "    \"\"\"\n",
    "    Calculate an important quantity that appears in the condition of tightness.\n",
    "\n",
    "    Parameters:\n",
    "    delta_M (float): The delta_M value representing the maximum l1 distance between transition probabilities (action-variation).\n",
    "    blackwell_gamma (float): The Blackwell optimal discount factor gamma.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated bias value.\n",
    "    \"\"\"\n",
    "    return (delta_M / 2) / (1 - blackwell_gamma * (1 - delta_M / 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the main cell which dictates parameters for the simulations for evaluating the tightness of theorem 1.\n",
    "\n",
    "# It first generates a list of MDPs (Markov Decision Processes) using the generate_mdps function (use fixed or ring).\n",
    "mdps = generate_mdps(100, 10, 3)\n",
    "\n",
    "#The number of samples for each state-action pair tested\n",
    "n_samples_list = [1, 5, 25, 50]\n",
    "\n",
    "#The gammas for which the bound is computed, precision can be changed as long as the discount factor is between ]0,1[.\n",
    "gammas = np.linspace(0.0001,0.99,20)\n",
    "\n",
    "all_estimated_mdps = []\n",
    "\n",
    "for i, n_samples in enumerate(n_samples_list):\n",
    "    estimated_mdps = estimate_mdps(mdps, n_samples)\n",
    "    all_estimated_mdps.append(estimated_mdps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell calculates the Blackwell optimal discount factor gamma for each MDP.\n",
    "# See the paper for a detailed explanation of the algorithm.\n",
    "gammas_for_bw = np.linspace(0.99, 0.0001, 100)  \n",
    "\n",
    "blackwell_gammas = np.zeros(len(mdps))\n",
    "\n",
    "def process_mdp(mdp_instance, gammas):\n",
    "    last_policy = None\n",
    "    for gamma_idx, gamma in enumerate(gammas_for_bw):\n",
    "        pi = mdptoolbox.mdp.PolicyIteration(mdp_instance.transitions, mdp_instance.rewards, discount=gamma)\n",
    "        pi.run()\n",
    "        if last_policy is not None and not np.array_equal(pi.policy, last_policy):\n",
    "            break\n",
    "        last_policy = pi.policy\n",
    "        gamma_value = gamma\n",
    "    return gamma_value\n",
    "\n",
    "# We use Joblib to parallelize this process since it can be very slow and every MDP is independent.\n",
    "results = Parallel(n_jobs=-1)(delayed(process_mdp)(mdp_instance, gammas) for mdp_instance in mdps)\n",
    "\n",
    "# Bringing the results in the right data structure.\n",
    "for idx, result in enumerate(results):\n",
    "    blackwell_gammas[idx] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell calculates if the condition is true for every mdps, number of sample and gammas.\n",
    "#We therefore start by declaring every parameter that goes in the condition\n",
    "kappa_gamma_true = np.zeros((len(gammas), len(mdps)))\n",
    "rmax = np.zeros((len(gammas), len(mdps)))\n",
    "epsilon = np.zeros((len(gammas),len(n_samples_list), len(mdps)))\n",
    "discordant_estimation = np.zeros((len(gammas),len(n_samples_list), len(mdps),mdps[0].num_states))\n",
    "discordant_horizon = np.zeros((len(gammas), len(mdps),mdps[0].num_states))\n",
    "kappa_gamma = np.zeros((len(gammas), len(mdps)))\n",
    "delta_M_gamma = np.zeros((len(gammas), len(mdps)))\n",
    "delta_M_gamma_hat = np.zeros((len(gammas), len(n_samples_list),len(mdps)))\n",
    "\n",
    "condition_reached = np.zeros((len(gammas), len(n_samples_list), len(mdps)))\n",
    "\n",
    "\n",
    "#We start by retrieving the optimal policy on the horizon\n",
    "policy_bw = []\n",
    "for mdp_idx, mdp in enumerate(mdps):\n",
    "    pi = mdptoolbox.mdp.PolicyIteration(mdp.transitions, mdp.rewards,discount=gammas[-1])\n",
    "    pi.run()\n",
    "    policy_bw.append(pi.policy)\n",
    "\n",
    "\n",
    "# This is the main loop. It iterates over each discount factor (gamma) and each MDP instance.\n",
    "# For each combination, it calculates various metrics such as kappa_gamma, rmax, discordant_horizon, and delta_M_gamma.\n",
    "# It then estimates the policy for different sample sizes and calculates the epsilon and discordant_estimation.\n",
    "# Finally, it checks the condition of tightness is met (see the paper for details)\n",
    "\n",
    "for gamma_idx, gamma in enumerate(gammas):\n",
    "    for mdp_idx, mdp in enumerate(mdps):\n",
    "        #Here we get the values for the true mdp with no estimation and we extract the policy\n",
    "        pi_true = mdptoolbox.mdp.PolicyIteration(mdp.transitions, mdp.rewards,discount=gamma)\n",
    "        pi_true.run()\n",
    "        true_policy_array = np.array(pi_true.policy)\n",
    "\n",
    "        kappa_gamma[gamma_idx, mdp_idx] = max(pi_true.V) - min(pi_true.V)\n",
    "        rmax[gamma_idx, mdp_idx] = np.max(mdp.rewards)\n",
    "        discordant_horizon[gamma_idx, mdp_idx, :] = (policy_bw[mdp_idx] != true_policy_array).astype(int)\n",
    "\n",
    "        delta_M_gamma[gamma_idx, mdp_idx] = calculate_delta_M(mdp, discordant_horizon[gamma_idx, mdp_idx, :])\n",
    "\n",
    "\n",
    "        for n_idx, n_samples in enumerate(n_samples_list):\n",
    "            \n",
    "            #Here we check for every estimated mdps. We start by retrieving the policy and then we extract the parameters\n",
    "            pi_estimated = mdptoolbox.mdp.PolicyIteration(all_estimated_mdps[n_idx][mdp_idx].transitions, all_estimated_mdps[n_idx][mdp_idx].rewards,discount=gamma)\n",
    "            pi_estimated.run()\n",
    "            estimated_policy_array = np.array(pi_estimated.policy)\n",
    "\n",
    "            #Get epsilon\n",
    "            R_pi_estimated = mdp.rewards[np.arange(mdp.num_states), estimated_policy_array]\n",
    "            P_pi_estimated = mdp.transitions[estimated_policy_array, np.arange(mdp.num_states), :]\n",
    "\n",
    "            R_pi_true = mdp.rewards[np.arange(mdp.num_states), true_policy_array]\n",
    "            P_pi_true = mdp.transitions[true_policy_array, np.arange(mdp.num_states), :]\n",
    "\n",
    "\n",
    "            V_pi_estimated = np.linalg.inv(np.eye(mdp.num_states) - gamma * P_pi_estimated) @ R_pi_estimated\n",
    "            V_pi_true = np.linalg.inv(np.eye(mdp.num_states) - gamma * P_pi_true) @ R_pi_true\n",
    "\n",
    "            epsilon[gamma_idx, n_idx, mdp_idx] = np.max(np.abs(V_pi_estimated - V_pi_true))\n",
    "\n",
    "            #Get discordant estimation\n",
    "            #For each states, flag a 1 if policies are unequal\n",
    "            discordant_estimation[gamma_idx, n_idx, mdp_idx, :] = (true_policy_array != estimated_policy_array).astype(int)\n",
    "\n",
    "            delta_M_gamma_hat[gamma_idx, n_idx, mdp_idx] = calculate_delta_M(mdp, discordant_estimation[gamma_idx, n_idx, mdp_idx, :])\n",
    "\n",
    "            condition_reached[gamma_idx, n_idx, mdp_idx] = (\n",
    "                epsilon[gamma_idx, n_idx, mdp_idx] <= \n",
    "                rmax[gamma_idx, mdp_idx] / (1 - gamma) - \n",
    "                kappa_gamma[gamma_idx, mdp_idx] * \n",
    "                (bias_ad_hoc_function(delta_M_gamma_hat[gamma_idx, n_idx, mdp_idx], blackwell_gammas[mdp_idx]) + \n",
    "                bias_ad_hoc_function(delta_M_gamma[gamma_idx, mdp_idx], blackwell_gammas[mdp_idx]))\n",
    "            )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the proportion of time for each gamma where condition_reached is True for each MDP\n",
    "plt.figure(figsize=(6, 3)) \n",
    "colors = ['blue', 'green', 'red', 'cyan']\n",
    "for i, n_samples in enumerate(n_samples_list):\n",
    "    proportions = np.zeros(len(gammas))\n",
    "    for gamma_index, gamma in enumerate(gammas):\n",
    "        proportions[gamma_index] = np.sum(condition_reached[gamma_index, i, :]) / condition_reached.shape[2]\n",
    "    plt.plot(gammas, proportions, linestyle='-', color=colors[i], label=f'{n_samples} samples per (s,a)')\n",
    "\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.ylabel('Proportion')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a vector image (PDF) with a larger size\n",
    "plt.savefig(\"gamma_proportions.pdf\", format='pdf', bbox_inches='tight')  \n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

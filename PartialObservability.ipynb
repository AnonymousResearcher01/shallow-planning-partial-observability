{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook for the paper titled \"Shallow Planning with Partial Observability\"\n",
    "\n",
    "# Install required packages\n",
    "!pip install numpy matplotlib pymdptoolbox joblib\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mdptoolbox\n",
    "import time\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting construction and partial observability structural parameters results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedMdp:\n",
    "    \"\"\"\n",
    "    A class to represent a fixed Markov Decision Process (MDP).\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    num_states : int\n",
    "        Number of states in the MDP.\n",
    "    num_actions : int\n",
    "        Number of actions in the MDP.\n",
    "    d : int\n",
    "        Number of possible next states for each state-action pair.\n",
    "    transitions : np.ndarray\n",
    "        Transition probabilities for the MDP.\n",
    "    rewards : np.ndarray\n",
    "        Rewards for the MDP.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    generate_mdp():\n",
    "        Generates the transition probabilities and rewards for an mdp of the type fixed. See the paper for details.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states=10, d=3):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = 2\n",
    "        self.d = d\n",
    "        self.transitions = np.zeros((self.num_actions, num_states, num_states))\n",
    "        self.rewards = np.zeros((num_states, self.num_actions))\n",
    "        self.generate_mdp()\n",
    "\n",
    "    def generate_mdp(self):\n",
    "        for state in range(self.num_states):\n",
    "            for action in range(self.num_actions):\n",
    "                #Each state can be connected to d other states. We just fill non empty entries with transitions and rewards.\n",
    "                probabilities = np.random.uniform(0, 1, self.d)\n",
    "                probabilities /= probabilities.sum()\n",
    "                next_states = np.random.choice(self.num_states, self.d, replace=False)\n",
    "                self.rewards[state, action] = np.random.uniform(0, 1)\n",
    "                self.transitions[action, state, next_states] = probabilities\n",
    "\n",
    "def generate_mdps(num_mdps=1000, num_states=10, d=3):\n",
    "    return [FixedMdp(num_states=num_states, d=d) for _ in range(num_mdps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingMdp:\n",
    "    \"\"\"\n",
    "    A class to represent a ring-shaped Markov Decision Process (MDP).\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    num_states : int\n",
    "        Number of states in the MDP.\n",
    "    p : float\n",
    "        Probability of additional random connections.\n",
    "    num_actions : int\n",
    "        Number of actions in the MDP.\n",
    "    transitions : np.ndarray\n",
    "        Transition probabilities for the MDP.\n",
    "    rewards : np.ndarray\n",
    "        Rewards for the MDP.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    generate_mdp():\n",
    "        Generates the transition probabilities and rewards for an mdp of type ring. See the paper for details.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states=10, p=0):\n",
    "        self.num_states = num_states\n",
    "        self.p = p\n",
    "        self.num_actions = 2\n",
    "        self.transitions = np.zeros((2, num_states, num_states))\n",
    "        self.rewards = np.zeros((num_states, 2))\n",
    "        self.generate_mdp()\n",
    "\n",
    "    def generate_mdp(self):\n",
    "        for state in range(self.num_states):\n",
    "            next_state_clockwise = (state + 1) % self.num_states\n",
    "            prev_state_counterclockwise = (state - 1) % self.num_states\n",
    "            \n",
    "            # Action 1: Move clockwise or stay\n",
    "            self.transitions[0, state, [next_state_clockwise, state]] = np.random.uniform(0, 1, 2)\n",
    "\n",
    "            # Action 2: Move counterclockwise or stay\n",
    "            self.transitions[1, state, [prev_state_counterclockwise, state]] = np.random.uniform(0, 1, 2)\n",
    "            \n",
    "            # Additional random connections based on probability p\n",
    "            for other_state in range(self.num_states):\n",
    "                if other_state != state and other_state != next_state_clockwise and np.random.rand() < self.p:\n",
    "                    self.transitions[0, state, other_state] = np.random.uniform(0, 1)\n",
    "                if other_state != state and other_state != prev_state_counterclockwise and np.random.rand() < self.p:\n",
    "                    self.transitions[1, state, other_state] = np.random.uniform(0, 1)\n",
    "            \n",
    "            # Normalize again after adding random connections\n",
    "            self.transitions[0, state, :] /= self.transitions[0, state, :].sum()\n",
    "            self.transitions[1, state, :] /= self.transitions[1, state, :].sum()\n",
    "            \n",
    "            # Generate rewards\n",
    "            self.rewards[state, 0] = np.random.uniform(0, 1)\n",
    "            self.rewards[state, 1] = np.random.uniform(0, 1)\n",
    "\n",
    "def generate_ring_mdps(num_mdps=1000, num_states=10, p=0):\n",
    "    return [RingMdp(num_states, p) for _ in range(num_mdps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDP:\n",
    "    \"\"\"\n",
    "    Partially Observable Markov Decision Process (POMDP) class.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mdp : RingMdp\n",
    "        The underlying Markov Decision Process (MDP).\n",
    "    num_observations : int\n",
    "        Number of possible observations.\n",
    "    num_actions : int\n",
    "        Number of possible actions.\n",
    "    p_o_s : np.ndarray\n",
    "        Observation probabilities given states.\n",
    "    p_s_o : np.ndarray\n",
    "        State probabilities given observations.\n",
    "    transitions : np.ndarray\n",
    "        Transition probabilities between observations.\n",
    "    rewards : np.ndarray\n",
    "        Rewards for each observation-action pair.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    define_observation_prob():\n",
    "        Defines the observation probabilities given states.\n",
    "    define_p_s_o():\n",
    "        Defines the state probabilities given observations.\n",
    "    define_transitions():\n",
    "        Defines the transition probabilities between observations.\n",
    "    define_rewards():\n",
    "        Defines the rewards for each observation-action pair.\n",
    "    transition_function(current_state, action):\n",
    "        Simulates a transition based on the MDP's transition probabilities.\n",
    "    observation_function(next_state, action):\n",
    "        Generates an observation based on the state's observation probability distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, mdp, num_observations):\n",
    "        \"\"\"\n",
    "        Initializes the POMDP with the given MDP and number of observations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mdp : RingMdp\n",
    "            The underlying Markov Decision Process (MDP).\n",
    "        num_observations : int\n",
    "            Number of possible observations.\n",
    "        \"\"\"\n",
    "        self.mdp = mdp\n",
    "        self.num_observations = num_observations\n",
    "        self.num_actions = mdp.num_actions\n",
    "        self.define_p_o_s()\n",
    "        self.define_p_s_o()\n",
    "        self.define_transitions()\n",
    "        self.define_rewards()\n",
    "\n",
    "    def define_p_o_s(self):\n",
    "        \"\"\"\n",
    "        Defines the observation probabilities given states.\n",
    "        \"\"\"\n",
    "        # Initialize P(O|S) for each state as a matrix of zeros\n",
    "        self.p_o_s = np.zeros((self.mdp.num_states, self.num_observations))\n",
    "        \n",
    "        # Assign each state to an observation ensuring every observation has at least one state\n",
    "        for state in range(self.mdp.num_states):\n",
    "            self.p_o_s[state, state % self.num_observations] = 1\n",
    "\n",
    "    def define_p_s_o(self):\n",
    "        \"\"\"\n",
    "        Defines the state probabilities given observations.\n",
    "        \"\"\"\n",
    "        # Initialize P(S|O) for each observation as a matrix of zeros\n",
    "        self.p_s_o = np.zeros((self.num_observations, self.mdp.num_states))\n",
    "        # Here for each observation we just define a uniform distribution on the corresponding ground states\n",
    "        for obs in range(self.num_observations):\n",
    "            ground_states = np.where(self.p_o_s[:, obs] == 1)[0]\n",
    "            num_ground_states = len(ground_states)\n",
    "            self.p_s_o[obs, :] = 0  # Set all probabilities to zero\n",
    "            self.p_s_o[obs, ground_states] = 1 / num_ground_states  # Set uniform probability for relevant ground states\n",
    "        \n",
    "    def define_transitions(self):\n",
    "        \"\"\"\n",
    "        Defines the transition probabilities between observations.\n",
    "        \"\"\"\n",
    "        self.transitions = np.zeros((self.num_actions, self.num_observations, self.num_observations))\n",
    "\n",
    "        for action in range(self.num_actions):\n",
    "            for observation in range(self.num_observations):\n",
    "                for next_observation in range(self.num_observations):\n",
    "                    self.transitions[action, observation, next_observation] = np.sum([np.dot(self.p_s_o[observation, :], self.mdp.transitions[action, :, next_state]) for next_state in range(self.mdp.num_states) if self.p_s_o[next_observation, next_state] > 0])\n",
    "\n",
    "    def define_rewards(self):\n",
    "        \"\"\"\n",
    "        Defines the rewards for each observation-action pair.\n",
    "        \"\"\"\n",
    "        self.rewards = np.einsum('os,sa->oa', self.p_s_o, self.mdp.rewards)\n",
    "\n",
    "    def transition_function(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Simulates a transition based on the MDP's transition probabilities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state : int\n",
    "            The current state in the MDP.\n",
    "        action : int\n",
    "            The action taken.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The next state in the MDP.\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.mdp.num_states, p=self.mdp.transitions[action, current_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up all the POMDPs (Partially Observable Markov Decision Processes) for the simulations.\n",
    "# It first generates a list of MDPs (Markov Decision Processes) using the generate_mdps function (use fixed or ring).\n",
    "mdps = generate_mdps(num_mdps=10000, num_states=10, d=3)\n",
    "\n",
    "# Define a list of omega values which will be used to create POMDPs with varying cardinality of the observation space (hence the name Omega).\n",
    "omega_values = [10, 8, 6, 4, 2, 1]\n",
    "\n",
    "# Initialize a 2D list to store all the POMDPs. The outer list corresponds to each MDP,\n",
    "# and the inner list corresponds to each omega value for that MDP.\n",
    "all_pomdps = [[None for _ in omega_values] for _ in mdps]\n",
    "\n",
    "# Iterate over each MDP and each omega value to create the corresponding POMDP.\n",
    "for mdp_idx, mdp in enumerate(mdps):\n",
    "    for p_idx, p in enumerate(omega_values):\n",
    "        pomdp = POMDP(mdp, p)\n",
    "        all_pomdps[mdp_idx][p_idx] = pomdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell calculates the action variation for each POMDPs and each underlying mdps.\n",
    "\n",
    "delta_M_o = np.zeros((len(all_pomdps), len(omega_values)))\n",
    "for pomdp_idx, pomdp_instance in enumerate(all_pomdps):\n",
    "    for p_idx, p in enumerate(omega_values):\n",
    "        for o in range(pomdp_instance[p_idx].num_observations):\n",
    "            if np.sum(np.abs(pomdp_instance[p_idx].transitions[0,o,:] - pomdp_instance[p_idx].transitions[1,o,:])) > delta_M_o[pomdp_idx, p_idx]:\n",
    "                delta_M_o[pomdp_idx, p_idx] = np.sum(np.abs(pomdp_instance[p_idx].transitions[0,o,:] - pomdp_instance[p_idx].transitions[1,o,:]))\n",
    "\n",
    "delta_M_s = np.zeros((len(all_pomdps)))\n",
    "for pomdp_idx, pomdp_instance in enumerate(all_pomdps):\n",
    "    for s in range(pomdp_instance[0].mdp.num_states):\n",
    "        if np.sum(np.abs(pomdp_instance[0].mdp.transitions[0,s,:] - pomdp_instance[0].mdp.transitions[1,s,:])) > delta_M_s[pomdp_idx]:\n",
    "            delta_M_s[pomdp_idx] = np.sum(np.abs(pomdp_instance[0].mdp.transitions[0,s,:] - pomdp_instance[0].mdp.transitions[1,s,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize by dividing the parameter on each POMDP by the one on the state space.\n",
    "normalized_delta_M_o = np.zeros((len(all_pomdps), len(omega_values)))\n",
    "for pomdp_idx, pomdp_instance in enumerate(all_pomdps):\n",
    "    for p_idx, p in enumerate(omega_values):\n",
    "        normalized_delta_M_o[pomdp_idx, p_idx] = delta_M_o[pomdp_idx, p_idx] / delta_M_s[pomdp_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell calculates the value-function variation for each POMDPs and their underlying MDPs.\n",
    "gammas = np.linspace(0.0001,0.99,3)\n",
    "\n",
    "state_values = np.zeros((len(gammas),len(all_pomdps), all_pomdps[0][0].mdp.num_states))\n",
    "#kappa_s\n",
    "kappa_gamma_s = np.zeros((len(gammas), len(all_pomdps)))\n",
    "for gamma_idx, gamma in enumerate(gammas):\n",
    "    for pomdp_idx, pomdp_instance in enumerate(all_pomdps):\n",
    "        pi = mdptoolbox.mdp.PolicyIteration(pomdp_instance[0].mdp.transitions, pomdp_instance[0].mdp.rewards,discount=gamma)\n",
    "        pi.run()\n",
    "        kappa_gamma_s[gamma_idx, pomdp_idx] = max(pi.V) - min(pi.V)\n",
    "        state_values[gamma_idx, pomdp_idx, :] = pi.V\n",
    "\n",
    "\n",
    "kappa_gamma_phi = np.zeros((len(gammas), len(all_pomdps), len(omega_values)))\n",
    "#for each pomdps find the optimal values for each p\n",
    "for gamma_idx, gamma in enumerate(gammas):\n",
    "    for pomdp_idx, pomdp_instance in enumerate(all_pomdps):\n",
    "        for p_idx, p in enumerate(omega_values):\n",
    "            pi = mdptoolbox.mdp.PolicyIteration(pomdp_instance[p_idx].transitions, pomdp_instance[p_idx].rewards,discount=gamma)\n",
    "            pi.run()\n",
    "            kappa_gamma_phi[gamma_idx, pomdp_idx, p_idx] = max(pi.V) - min(pi.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell calculates the bias for every POMDPs for varying values of gamma.\n",
    "\n",
    "#You can set up the precision by changing the number of points in the linspace as long as the discount factor is between ]0,1[.\n",
    "gammas_biases = np.linspace(0.0001,0.99,100)\n",
    "\n",
    "bias = np.zeros((len(gammas_biases), len(all_pomdps), len(omega_values)))\n",
    "\n",
    "# Now calculate the relative losses\n",
    "for p_idx, p in enumerate(omega_values):\n",
    "    for pomdp_idx, pomdp_instance in enumerate(all_pomdps):\n",
    "        for gamma_idx, gamma in enumerate(gammas_biases):\n",
    "            if gamma_idx == 0:\n",
    "                pi = mdptoolbox.mdp.PolicyIteration(pomdp_instance[p_idx].transitions, pomdp_instance[p_idx].rewards, discount=gammas_biases[-1])\n",
    "                pi.run()\n",
    "                Max_values = pi.V\n",
    "            pi = mdptoolbox.mdp.PolicyIteration(pomdp_instance[p_idx].transitions, pomdp_instance[p_idx].rewards, discount=gamma)\n",
    "            pi.run()\n",
    "            policy_array = np.array(pi.policy)\n",
    "            R_pi = pomdp_instance[p_idx].rewards[np.arange(pomdp_instance[p_idx].num_observations), policy_array]\n",
    "            P_pi = pomdp_instance[p_idx].transitions[policy_array, np.arange(pomdp_instance[p_idx].num_observations), :]\n",
    "            V_pi_lowgamm = np.linalg.inv(np.eye(pomdp_instance[p_idx].num_observations) - gammas_biases[-1] * P_pi) @ R_pi\n",
    "            bias[gamma_idx, pomdp_idx, p_idx] = np.max((Max_values - V_pi_lowgamm) / Max_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the value-function variation for each POMDPs by the value of the parameter on the underlying MDP.\n",
    "normalized_kappa_gamma = np.zeros((len(gammas), len(all_pomdps), len(omega_values)))\n",
    "for gamma_idx, gamma in enumerate(gammas):\n",
    "    for pomdp_idx, pomdp_instance in enumerate(all_pomdps):\n",
    "        for p_idx, p in enumerate(omega_values):\n",
    "            normalized_kappa_gamma[gamma_idx, pomdp_idx, p_idx] = kappa_gamma_phi[gamma_idx, pomdp_idx, p_idx] / kappa_gamma_s[gamma_idx, pomdp_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell calculates the Blackwell optimal discount factor gamma for each POMDP and omega value.\n",
    "# See the paper for a detailed explanation of the algorithm.\n",
    "gammas_for_bw = np.linspace(0.99, 0.0001, 100)  \n",
    "\n",
    "blackwell_gammas = np.zeros((len(all_pomdps), len(omega_values)))\n",
    "\n",
    "def process_pomdp(pomdp_instance, p_idx, p, gammas):\n",
    "    last_policy = None\n",
    "    for gamma_idx, gamma in enumerate(gammas_for_bw):\n",
    "        pi = mdptoolbox.mdp.PolicyIteration(pomdp_instance[p_idx].transitions, pomdp_instance[p_idx].rewards, discount=gamma)\n",
    "        pi.run()\n",
    "        if last_policy is not None and not np.array_equal(pi.policy, last_policy):\n",
    "            break\n",
    "        last_policy = pi.policy\n",
    "        gamma_value = gamma\n",
    "    return gamma_value\n",
    "\n",
    "#We use Joblib to paralellize this process since it can be very slow and every POMDPs are independant.\n",
    "results = Parallel(n_jobs=-1)(delayed(process_pomdp)(pomdp_instance, p_idx, p, gammas) for pomdp_idx, pomdp_instance in enumerate(all_pomdps) for p_idx, p in enumerate(omega_values))\n",
    "\n",
    "#Bringing the results in the right data structure.\n",
    "for idx, result in enumerate(results):\n",
    "    pomdp_idx = idx // len(omega_values)\n",
    "    p_idx = idx % len(omega_values)\n",
    "    blackwell_gammas[pomdp_idx, p_idx] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell plots the data for the blackwell discount factor and the bias which varies as a function of Omega\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 4), gridspec_kw={'width_ratios': [1, 1], 'wspace': 0.25})  # Create a figure with 1 row and 2 columns, increased figure width and added space between plots\n",
    "\n",
    "# Common settings for all plots\n",
    "plot_settings = {\n",
    "    'showmeans': True,\n",
    "    'showextrema': True,\n",
    "    'showmedians': True\n",
    "}\n",
    "\n",
    "#Plotting the blackwell discount factor\n",
    "axs[0].violinplot(blackwell_gammas, positions=range(len(omega_values)), showmeans=True, showextrema=True, showmedians=True)\n",
    "axs[0].set_xticks(range(len(omega_values)))\n",
    "axs[0].set_xticklabels(omega_values)\n",
    "axs[0].set_ylabel('$\\gamma_{Bw}$', fontsize=11)\n",
    "axs[0].set_xlabel('Number of abstracted states $|\\Omega|$', fontsize=11)\n",
    "\n",
    "\n",
    "#Plotting the bias\n",
    "for p_idx, p in enumerate(omega_values):\n",
    "    axs[1].plot(gammas_biases, bias[:, :, p_idx].mean(axis=1), label=p)\n",
    "axs[1].set_ylabel('Average normalized bias', fontsize=11)\n",
    "axs[1].legend(title=\"$|\\Omega|$\", fontsize='small')  \n",
    "axs[1].set_xlabel('$\\gamma$', fontsize=11)\n",
    "\n",
    "\n",
    "# Save the figure as a PDF file\n",
    "plt.savefig(\"Combined_Plots_bias.pdf\", format='pdf')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell generates the plots for the normalized value-function variation and the normalized action variation for different values of\n",
    "# Omega.\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 4))  # Create a figure with 1 row and 2 columns\n",
    "\n",
    "#We start by plotting the normalized value-function variation for different values of Omega.\n",
    "violin_parts = []\n",
    "offset_increment = 0.2\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "    positions = [p + i * offset_increment for p in range(len(omega_values))]  \n",
    "    vp = axs[0].violinplot(normalized_kappa_gamma[i], positions=positions, **plot_settings, widths=0.3)  \n",
    "    violin_parts.append(vp)\n",
    "\n",
    "axs[0].set_xticks([p + (len(gammas) - 1) * 0.2 / 2 for p in range(len(omega_values))])  \n",
    "axs[0].set_xticklabels(omega_values)\n",
    "axs[0].set_ylabel(r'Normalized value-function variation $\\frac{\\kappa_{M,\\gamma}^\\phi}{\\kappa_{M,\\gamma}^S}$', fontsize=11)\n",
    "axs[0].set_xlabel('Number of abstracted states $|\\Omega|$', fontsize=11)\n",
    "labels = [f'$\\gamma$ = {gamma:.2f}' for gamma in gammas]\n",
    "axs[0].legend([vp[\"bodies\"][0] for vp in violin_parts], labels, loc='upper right', fontsize=10)\n",
    "\n",
    "\n",
    "#Next we plot the normalized action variation for different values of Omega.\n",
    "axs[1].violinplot(normalized_delta_M_o, positions=range(len(omega_values)), **plot_settings)\n",
    "axs[1].set_xticks(range(len(omega_values)))\n",
    "axs[1].set_xticklabels(omega_values)\n",
    "axs[1].set_ylabel(r'Normalized action variation $\\frac{\\delta_M^\\phi}{\\delta_M^S}$', fontsize=11)\n",
    "axs[1].set_yticks(np.arange(0, 1.1, 0.5))\n",
    "axs[1].set_xlabel('Number of abstracted states $|\\Omega|$', fontsize=11)\n",
    "\n",
    "plt.savefig(\"Combined_Plots_params.pdf\", format='pdf')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
